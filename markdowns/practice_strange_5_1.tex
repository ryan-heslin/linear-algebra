% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Section 5.1 Problems},
  pdfauthor={Ryan Heslin},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{255,255,255}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{\textbf{\colorbox[rgb]{0.97,0.90,0.90}{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.79,0.38,0.79}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.00,0.34,0.68}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.69,0.50,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.39,0.29,0.61}{\textbf{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.57,0.30,0.62}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.54,0.53,0.53}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.00,0.58,1.00}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.67,0.33,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.12,0.11,0.11}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.00,0.34,0.68}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.69,0.50,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.38,0.47,0.50}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{\underline{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.58,1.00}{\textbf{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.69,0.50,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.39,0.29,0.61}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{1.00,0.33,0.00}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.69,0.50,0.00}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.12,0.11,0.11}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.12,0.11,0.11}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.12,0.11,0.11}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.43,0.16}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.00,0.43,0.16}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.34,0.68}{\colorbox[rgb]{0.88,0.91,0.97}{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.24,0.68,0.91}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{1.00,0.33,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.34,0.68}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{#1}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Section 5.1 Problems}
\author{Ryan Heslin}
\date{}

\begin{document}
\maketitle

\newcommand{\abcd}{\begin{bmatrix}a&b\\
c&d\end{bmatrix}}

\newcommand{\m}[1]{\begin{bmatrix}#1\end{bmatrix}}

\newcommand{\vect}[1]{\begin{pmatrix}#1\end{pmatrix}}

\newcommand{\meq}[1]{\begin{split}#1\end{split}}

\newcommand{\bym}[1]{#1\times{m}}

\newcommand{\nby}[1]{n\times{#1}}

\newcommand{\subsp}[2]{\Bigg\{\begin{bmatrix}#1\end{bmatrix}:#2\Bigg\}}

\newcommand{\proj}[2]{\text{proj}_#1(#2)}

\newcommand{\refl}[2]{\text{refl}_#1(#2)}

\newcommand{\sumn}{\sum_{i=1}^n}

\newcommand{\dotsn}[5]{#1_{1}#3_{1}#5{#1}_{2}#3_{2}{#5}\dots{#5}#1_{#2}#3_{#4}}

\hypertarget{section}{%
\subsection{1.}\label{section}}

\textless!--Just find the eigens:

\[A = \begin{bmatrix}
  1 & -1\\
  2 & 4\\
\end{bmatrix}\]

\[
  \begin{vmatrix}
    1 - \lambda & 2\\
    2 & 4 - \lambda\\
  \end{vmatrix}x = 0
\]

\[
  \begin{aligned}
    & \lambda^2 -5\lambda +6 = 0\\
    & (\lambda -3)(\lambda -2) = 0\\
    & \lambda_1 = 2 \quad \lambda_2 = 3
  \end{aligned}
\]

\[x_1 = \begin{bmatrix}
  1 & -1\\
  2 & 4\\
\end{bmatrix} - \begin{bmatrix}
  2 & 0\\
  0 & 2\\
\end{bmatrix}  = \begin{bmatrix}
  -1 & -1\\
  2  & 2\\
\end{bmatrix} = \begin{bmatrix}
  -1\\
  1\\
\end{bmatrix}\]

The second eigenvector is\[\begin{bmatrix}
  -1/2\\
  1\\
\end{bmatrix}\]

Sure enough, \(3+2 =1 +4=5\) and \(3(2) =4(1) - (-1)(2)=6\).

\hypertarget{section-1}{%
\subsection{3.}\label{section-1}}

The values are shifted down 7, so \(\lambda_1 = -5\) and
\(\lambda_2= -4\). But the vectors remain the same.

\hypertarget{section-2}{%
\subsection{5.}\label{section-2}}

\[A = \begin{bmatrix}
  3 & 4 & 2\\
  0 & 1 & 2\\
  0 & 0 & 0\\
\end{bmatrix}\] \[(3-\lambda)(1 -\lambda)(0) = 0\]
\[\lambda_1 = 3\quad\lambda_2 =1\quad\lambda_3 = 0 \]

{[}

\begin{verbatim}
& \begin{bmatrix}
  0 & 4 & 2\\
  0 & -2 & 2\\
  0 & 0 & -3
\end{bmatrix}\implies x_1 = \begin{bmatrix}
  1\\
  0\\
  0\\
\end{bmatrix}
\end{verbatim}

{]}

\[A - \lambda_2I = \begin{bmatrix}
  2 & 4 & 2\\
  0 & 0 & 2\\
  0 & 0 & -1\\
\end{bmatrix} \implies x_2 = \begin{bmatrix}
  -2\\
  1\\
  0\\
\end{bmatrix}\]

And for the last eigenvalue 0, just the kernel of \(A\) itself,
\[\begin{bmatrix}
  2\\
  0\\
  1\\
\end{bmatrix}
\] For \(B\), the determinant is -8, since it takes a row swap to
diagonalize :

The eigenvalues must sum to 2 (the trace) and have a product of -8,
which implies they are \(\lambda = (2, 2, -2)\). So the vectors are

\[
  \begin{aligned}
    & \begin{bmatrix}
      -2 & 0 & 2\\
      0 & 0 & 0\\
      2 & 0 & -2\\
    \end{bmatrix} \implies x_1 = \begin{bmatrix}
      1\\
      0\\
      1\\
    \end{bmatrix}
  \end{aligned}
\]

and

\[
  \begin{aligned}
    & \begin{bmatrix}
      2 & 0 & 2\\
      0 & 4 & 0\\
      2 & 0 & 2\\
    \end{bmatrix} \implies x_2 = \begin{bmatrix}
      -1\\
      0\\
      1\\
    \end{bmatrix}
  \end{aligned}
\]

\hypertarget{section-3}{%
\subsection{6.}\label{section-3}}

If we consider \(A = \begin{bmatrix}1 & 2\\4 & 6\end{bmatrix}\) and
\(B = \begin{bmatrix}1 & 2\\3 &4\end{bmatrix}\), the characteristic
polynomials \(\lambda^2 -7\lambda -2\) and \(\lambda^2 - 5\lambda -2\)
are clearly different. We have subtracted 2 from the trace, hence the
difference. Zero eigenvalues are not changed by elimination because they
represent a vector that \emph{already} solves \(Ax=0x=0\). Because row
operations do not distrurb the kernel, since they respect row space,
zero eigenvalues remain unchanged.

\hypertarget{section-4}{%
\subsection{7.}\label{section-4}}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  If \(B = A - 7I\), algebra shows that:
\end{enumerate}

\[
  \begin{aligned}
    & Ax = \lambda x\\
    & (A-7I)x = \lambda x\\
    & Ax -7x = \lambda x\\
    & Ax = \lambda x + 7x \\
    & Ax = (\lambda + 7)x
  \end{aligned}
\]

So the eigenvalue for \(A\) is 7 greater than for \(B\).

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \(\det(A^{-1}) = 1/ \det(A)\)
\end{enumerate}

\[
  \begin{aligned}
    & Ax = \lambda x\\
    & (A^{-1})^2Ax=(A^{-1})^2 \lambda x\\
    & A^{-1}x = A^{-1}x
  \end{aligned}
\]

\hypertarget{section-5}{%
\subsection{8.}\label{section-5}}

If we set \(\lambda\) to 0\ldots{} \[
  \begin{aligned}
    & \det(A - \lambda I) = (\lambda_1 - \lambda)(\lambda_2 - \lambda) \dots (\lambda_n - \lambda)\\
    & \det(A) = \prod_{i=1}^n\lambda_i
  \end{aligned}
\]

\hypertarget{section-6}{%
\subsection{9.}\label{section-6}}

\[
  \begin{aligned}
    & (-1)^n\lambda^n +(-1)^{n-1}(trA)\lambda^{n-1}+\dots + \det A=0\\
    &-\lambda^n +(trA)(-\lambda)^{n-1} + \text{some convoluted polynomial} = 0\\
    & -\lambda^{n-1}(-\lambda + tr(A)) + \dots =0
  \end{aligned}
\]

\hypertarget{section-7}{%
\subsection{10.}\label{section-7}}

\[A=  \begin{bmatrix}
  2 & 0\\
  0 & 3
\end{bmatrix} \quad B = \begin{bmatrix}
  1 & 1\\
  1 & 1
\end{bmatrix}\]

\[\lambda_A = 2,3 \quad \lambda_b = 0, 2 \quad \lambda_{A+B} = \frac{7 \pm \sqrt{3}}{2}, \quad \lambda_{AB} = 0, 5\]

As we'd expect, \(2 + 3 + 0 + 2=7\) and \$ 2 \cdot 3 \cdot 0 \cdot 2 = 0
\cdot 5\$, since

\[(A + B)x = \lambda x \implies Ax + Bx = \lambda x\]

\hypertarget{section-8}{%
\subsection{11.}\label{section-8}}

The eigen \emph{values} of \(A\) and \(A^T\) are the same because
\(\det(A^T) = \det(A)\), and the trace remains constant as well. But the
vectors are different. Consider
\(\begin{bmatrix}1 &2\\0 & 0\end{bmatrix}\) and
\(\begin{bmatrix}1& 0\\ 2& 0\end{bmatrix}\). The eigenvalues are both 1
and 0, but the vectors are very different:
\((\begin{bmatrix}1\\0\end{bmatrix}, \begin{bmatrix}-2\\1\end{bmatrix})\)
for the first,
\((\begin{bmatrix}0\\1\end{bmatrix}, \begin{bmatrix}1/2\\1\end{bmatrix})\)
for the second.

\hypertarget{section-9}{%
\subsection{12.}\label{section-9}}

If \(A = \begin{bmatrix}0 & 1\\ 1 & 0\end{bmatrix}\) and
\(B = \begin{bmatrix}2 & 0\\0 &2\end{bmatrix}\), then for \(A\) the
eigens are \(\pm1\), for \(B\) both are 2. But for
\(A + B = \begin{bmatrix}2 & 1\\ 1 & 2\end{bmatrix}\), the eigenvalues
are 3 and 1. But \(3 +1 = -1 + 1 + 2 + 2 = 4\), while

\hypertarget{section-10}{%
\subsection{12.}\label{section-10}}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
\end{enumerate}

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      3 & 4\\
      4 & -3\\
    \end{bmatrix}\\
    & (3 - \lambda)(-3 - \lambda) -4(4) = 0\\
    & \lambda^2 - 15 = 0\\
    & \lambda_1 = 5, \lambda_2 = -5
  \end{aligned}
\]

For the vectors:

\[
  \begin{aligned}
    & A - \lambda_1I = \begin{bmatrix}
      -2 & 4\\
      4 & -8\\
    \end{bmatrix} \implies x_1 = \begin{bmatrix}
      2\\
      1\\
    \end{bmatrix}
  \end{aligned}
\]

\[
  \begin{aligned}
    & A - \lambda_2I = \begin{bmatrix}
      8 & 4\\
      4 & 2\\
    \end{bmatrix} \implies x_2 = \begin{bmatrix}
      -1/2\\
      1\\
    \end{bmatrix}
  \end{aligned}
\]

For the general \(\begin{bmatrix}1 & b\\ b & 1\end{bmatrix}\) case, the
polynomial works out:

\[
  \begin{aligned}
    & (a-\lambda)(a-\lambda) - b^2 = 0\\
    & \lambda^2 -2a \lambda - b^2 = 0
  \end{aligned}
\]

We resort to the quadratic formula and get a nice simplification:

\[
  \begin{aligned}
    & \lambda = \frac{2a \pm \sqrt{(2a)^2 - 4(a^2 - b^2)}}{2}\\
    & =\frac{2a \pm \sqrt{4b^2}}{2}\\
    & = \frac{2a \pm 2b}{2}\\
    & = a \pm b
  \end{aligned}
\]

\hypertarget{section-11}{%
\subsection{13.}\label{section-11}}

Given this, \(tr(A) = 1 +2 +3 + 7 + 8 +9= 30\)

\hypertarget{section-12}{%
\subsection{14.}\label{section-12}}

Given this matrix:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mat2latex}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{16}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\[
\begin{bmatrix}
1 & 1 & 1 & 1\\
1 & 1 & 1 & 1\\
1 & 1 & 1 & 1\\
1 & 1 & 1 & 1
\end{bmatrix}
\]

rank is clearly 1. Three eigenvalues are 0, and the remaining one is 4,
because \(\sum \lambda = \text{trace}(A)\)

For the matrix \[
\begin{bmatrix}
  0 & 1 & 0 & 1\\
  1 & 0 & 1 & 0\\
  0 & 1 & 0 & 1\\
  1 & 0 & 1 & 0\\
\end{bmatrix}
\] rank is 2, meaning two eigenvalues are 0. The other two are
\(\pm 2\).

\hypertarget{section-13}{%
\subsection{15.}\label{section-13}}

For the first matrix in the previous problem, if it is \(n \times n\),
then \(n-1\) eigenvalues are 0 and the reamining one is \(n\). In the
second matrix, all but 2 eigenvalues are 0, and the remaining 2 are
\(\pm n -2\)

\hypertarget{section-14}{%
\subsection{16.}\label{section-14}}

If the matrix is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mat2latex}\NormalTok{(}\FunctionTok{square}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{16}\NormalTok{)) }\SpecialCharTok{{-}} \FunctionTok{diag}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\[
\begin{bmatrix}
0 & 1 & 1 & 1\\
1 & 0 & 1 & 1\\
1 & 1 & 0 & 1\\
1 & 1 & 1 & 0
\end{bmatrix}
\]

All the eigenvalues are shifted down 1, so three are now -1 and the
remaining one is 3 (in order to sum to a trace of 0).

\hypertarget{section-15}{%
\subsection{18.}\label{section-15}}

\(A\) has eigenvalues \((0, 3, 5)\) with eigenvectors \(u, v, w\).

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Then \(u\) is a basis for the kernel because \(Au = 0u\) by
  definition. The other two eigenvectors provide a basis for the image.
\item
  \(Ax =u\) has no solution because \(Au = \lambda u = 0u = u\). All
  multiples of \(u\) are in the kernel.
\end{enumerate}

\hypertarget{section-16}{%
\subsection{19.}\label{section-16}}

The eigenvalues of \(A\) are \((1, 1/2)\) and of \(A^2\) \((1, 1/4)\).
Since \(A^n = S \Lambda^n S^{-1}\) and \((1/2)^2 = 1/2(1/2) = 1/4\),
halving \(A + A^{\infty}\) completes one-haldf of the exponential decay
the eigenvalue \(1/2\) has to undergo before reaching 0 at the steady
state.

\hypertarget{section-17}{%
\subsection{20.}\label{section-17}}

\(A +I\) has the same eigenvectors as \(A\), but its eigenvalues are
increased by 1.

The eigenvalues are

\[
  \begin{aligned}
    & (1- \lambda)(3 - \lambda) - 2(4) = 0\\
    & \lambda^2 - 4 \lambda - 8 =0\\
    & \frac{4 \pm \sqrt{16 - 4(1)(-8)}}{2(-4)}\\
    & \frac{4 \pm 4 \sqrt{3}}{2}\\
    & 2 \pm 2 \sqrt{3}
  \end{aligned}
\]

Not going to bother working out the vectors.

\hypertarget{section-18}{%
\subsection{21.}\label{section-18}}

The eigenvalues of \(A^{-1}\) are the inverse of those of \(A\), and its
eigenbasis is the inverse of \(A\)'s eigenbasis.

\hypertarget{section-19}{%
\subsection{22.}\label{section-19}}

\(A^2\) has the same eigenvectors as \(A\), but its eigenvalues are
squared (\(\lambda^2 = 9, 4\)).

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      -1 & 3\\
      2 & 0\\
    \end{bmatrix}\\
    & \lambda^2 + \lambda - 6 = 0\\
    & \lambda = -3, 2\\
    & S = \begin{bmatrix}
      -3/2 & 1\\
      1& 1\\
    \end{bmatrix}
  \end{aligned}
\]

\hypertarget{section-20}{%
\subsection{23.}\label{section-20}}

If you know an eigenvector, find \(\lambda\) by multiplying it by \(A\).

If you know an eigenvalue, find the eigenvector by subtracting
\(\lambda I\) from \(A\) and finding a basis for the kernel.

\hypertarget{section-21}{%
\subsection{24.}\label{section-21}}

Some proofs.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
\end{enumerate}

\[
  \begin{aligned}
    & Ax = \lambda x\\
    & A^2x = A \lambda x\\
    & A^2 x = \lambda Ax\\
    & A^2x = \lambda^2 x
  \end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \[
    \begin{aligned}
   & Ax = \lambda x\\
   & (A^{-1})^2Ax =(A^{-1})^2 \lambda x\\
   & A^{-1} x = (A^{-1})^2\lambda x\\
   & A^{-1} x = \lambda  (\lambda^{-1})^2 x\\
   & = \lambda ^{-1}x
    \end{aligned}
  \]
\item
  \$\$

  \begin{aligned}

   & Ax + x = \lambda x +x \\
   & (A +I)x = \lambda_x + x\\
   &= (\lambda +1)x
    \end{aligned}

  \$\$
\end{enumerate}

\hypertarget{section-22}{%
\subsection{25.}\label{section-22}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}\SpecialCharTok{/}\DecValTok{6}

\NormalTok{P }\OtherTok{\textless{}{-}} \FunctionTok{tcrossprod}\NormalTok{(u)}
\FunctionTok{mat2latex}\NormalTok{(}\FunctionTok{round}\NormalTok{(P }\SpecialCharTok{\%*\%}\NormalTok{ u, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\[
\begin{bmatrix}
0.167\\
0.167\\
0.5\\
0.833
\end{bmatrix}
\]

Since \(P\) only has rank 1, and is symmetric, an orthogonal vector with
\(\lambda = 0\) would be \(\begin{bmatrix}-1 1\\0\\0\end{bmatrix}\).

The three eigenvectors with \(\lambda = 0\) are the bases of
\(A^{\perp}\), which is also the kernel since \(A\) is symmetric.
\(A^T\) reduces to one row \(\begin{bmatrix}1 & 1 & 3 &5\end{bmatrix}\),
which implies a kernel basis of

\[ \begin{bmatrix}
  -1 & 3 & 5\\
  1 & 0 & 0\\
  0 & 1 & 0\\
  0 & 0 & 1\\
\end{bmatrix}\]

\hypertarget{section-23}{%
\subsection{26.}\label{section-23}}

Given the rotation matrix

\[Q = \begin{bmatrix}
  \cos \theta & -\sin \theta\\
  \sin \theta & cos \theta\\
\end{bmatrix}\]

The determinant works out to

\[
  \begin{aligned}
    & \det(Q - \lambda I) = (\cos - \lambda)^2 -(\sin)(-\sin)\\
    & = \cos^2 + \lambda^2 - 2 \lambda \cos - \sin^2
  \end{aligned}
\]

\hypertarget{section-24}{%
\subsection{27.}\label{section-24}}

Every permutation matrix has an eigenvalue of 1 (for a vector fo ones).
We can find others.

For

\[ \begin{bmatrix}
  0 & 1 & 0\\
  0 & 0 & 1\\
  1 & 0 & 0\\
\end{bmatrix}\] determinant is 1 (two row swaps from the identity, and
trace 0, so \(1 + \lambda_2 + \lambda_3 =0\) and
\(1\times \lambda_2 \lambda_3=1\)). Since trace is 0, the characterictic
polynomial reduces to \(- \lambda^3 + 0 + 1/2(0) + 1=0\). SO
\(\lambda^3 = 1\). FO the others:

\[
  \begin{aligned}
    & \lambda(\lambda^2 + \lambda +1) = 0)
  \end{aligned}
\] By the quadratic formula:

\[\lambda = \frac{-1 \pm i \sqrt3}{2}\]

So there are two complex roots. For \[ \begin{bmatrix}
  0 & 0 & 1\\
  0 & 1 & 0\\
  1 & 0 & 0\\
\end{bmatrix}\] determinant is -1 (1 row swap) and trace is 1. This
implies the remaining eigenvalues are \(\pm 1\).

\hypertarget{section-25}{%
\section{28.}\label{section-25}}

Technically correct, though lazy. \[
 \begin{bmatrix}
  4 & 0\\
  0 & 5
\end{bmatrix} \quad
\begin{bmatrix}
  5 & -1000000\\
  0 & 4
\end{bmatrix}
\begin{bmatrix}
  5 & 0\\
  3 & 4
\end{bmatrix}
\]

\hypertarget{section-26}{%
\subsection{29.}\label{section-26}}

We know the rank of \(B\) (2), the determinant of \(B^TB\)
(\(\det(B)^2 = 0^2 =0)\), and the eigenvalues of \(B + I)^{-1}\) (1,
1/2, and 1/3). We don't know the eigenvalues of \(B^TB\).

\hypertarget{section-27}{%
\subsection{30.}\label{section-27}}

Given \[\begin{bmatrix}
  0 & 1\\
  c & d\\
\end{bmatrix}\]

\(c = 4+7 = 11\) and \(0(1) -d=28\) so \(d =-28\).

\hypertarget{section-28}{%
\subsection{31.}\label{section-28}}

\(c=0\) so the trace sums to 0. \(a = 0\) and \(b=9\) so the determinant
remains 0 no matter which eigenvalue is subtracted; those choices make
row 3 a multiple of 2 if a nonzero. eigenvalue is subtracted and of row
1 and 2 if the eigenvalue is 0.

\hypertarget{section-29}{%
\subsection{32.}\label{section-29}}

\(\lambda = (0, 1, -1/2)\) because all Markovs have 1 as an eigenvalue,
singularity means 0 is an eigenvalue, and the eigenvalues must sum to
the trace. \#\# 33.

Three such matrices are: \[
 \begin{bmatrix}
  0 & 0\\
  0 & 0\\
\end{bmatrix},
 \begin{bmatrix}
   -1 & -1\\
    1 & 1\\
 \end{bmatrix},
 \begin{bmatrix}
   2 & -2\\
   2 & -2\\
 \end{bmatrix}
\] \#\# 34.

Given rank 1, two eigenvalues are 0, leaving one as six.

The eigenvectors:

\[A = \begin{bmatrix}
  1 & -1 & 0\\
  2 & 1 & -2\\
  1 & 1/2 & 1
\end{bmatrix}\]

\hypertarget{section-30}{%
\subsection{35.}\label{section-30}}

\(Ax = Bx = \lambda x\) \#\# 36.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \(\lambda = (1, 4, 6)\)
\item
  \(\lambda = (2, \pm \sqrt 6)\)
\item
  \(\lambda = (0, 0, 6)\)
\end{enumerate}

\hypertarget{section-31}{%
\subsection{37.}\label{section-31}}

Given \(a+b =c +d\),
\(A\begin{bmatrix}1\\1\end{bmatrix} = \begin{bmatrix}1 +b\\c+d\end{bmatrix}\),
which means \(\lambda x\) has equal values and is therefore a scalar
multiple of \(x\). Then the eigenvalues are \(a+b\) and \(c +d\), which
must be equal.

\hypertarget{section-32}{%
\subsection{39.}\label{section-32}}

Yes. This is easy to show geometrically, the matrix of a 120-degree
rotation cubed gives the identity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{4} \SpecialCharTok{*}\NormalTok{ pi)}\SpecialCharTok{/}\DecValTok{3}

\FunctionTok{mat2latex}\NormalTok{(}\FunctionTok{round}\NormalTok{(matador}\SpecialCharTok{::}\FunctionTok{square}\NormalTok{(}\FunctionTok{cos}\NormalTok{(theta), }\FunctionTok{sin}\NormalTok{(theta), }\SpecialCharTok{{-}}\FunctionTok{sin}\NormalTok{(theta),}
    \FunctionTok{cos}\NormalTok{(theta)) }\SpecialCharTok{\%\^{}\%} \DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\[
\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}
\]

\hypertarget{section-33}{%
\subsection{40.}\label{section-33}}

For permutation matrices in general, determinants are \(\pm 1\)
(depending on the number of swaps), the trace varies from 0 to 3, and
the possible eigenvalues are \(\pm1\) and nasty complex roots of the
characteristic polynomial. Pivots are always 1 or 0.

\end{document}
