---
title: "Section 6.3 Problems"
author: "Ryan Heslin"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  pdf_document:
    highlight: "kate"
    df_print: "kable"
---

 <!-- Standard custom LaTeX commands -->
\newcommand{\abcd}{\begin{bmatrix}a&b\\
c&d\end{bmatrix}}
\newcommand{\m}[1]{\begin{bmatrix}#1\end{bmatrix}}

\newcommand{\vect}[1]{\begin{pmatrix}#1\end{pmatrix}}

\newcommand{\meq}[1]{\begin{split}#1\end{split}}

\newcommand{\bym}[1]{#1\times{m}}

\newcommand{\nby}[1]{n\times{#1}}

\newcommand{\subsp}[2]{\Bigg\{\begin{bmatrix}#1\end{bmatrix}:#2\Bigg\}}

\newcommand{\proj}[2]{\text{proj}_#1(#2)}

\newcommand{\refl}[2]{\text{refl}_#1(#2)}

\newcommand{\sumn}{\sum_{i=1}^n}

<!-- % 1: term 1 -->
<!-- % 2: subscript 1 -->
<!-- % 3: term 2 -->
<!-- % 4: subscript 2 -->
<!-- % 5. operation -->
\newcommand{\dotsn}[5]{#1_{1}#3_{1}#5{#1}_{2}#3_{2}{#5}\dots{#5}#1_{#2}#3_{#4}}

\newcommand{\rot}[1]{
\begin{bmatrix}
\cos{(#1)} & -\sin{(#1)}\\

\sin{(#1)} & \cos{(#1)}\\
\end{bmatrix}
}
```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = TRUE,
  comment = "",
  fig.pos = "",
  warning = FALSE,
  tidy = TRUE,
  fig.align = "center",
  highlight = TRUE
)

library(tidyverse)
library(rlang)
library(matador)
```

## 1.

A simple SVD.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      1 & 4\\
      2 & 8
    \end{bmatrix}\\&
    A^TA = \begin{bmatrix}
      5 & 20\\
      20 & 80
    \end{bmatrix}\\
    & AA^T = \begin{bmatrix}
      17 & 34\\
      34 & 68
    \end{bmatrix}
    & A =  \frac{1}{\sqrt 5} \begin{bmatrix}
      1\\
      2
    \end{bmatrix}
    \begin{bmatrix}
      \sqrt 85
    \end{bmatrix} 
    \begin{bmatrix}
      1/ \sqrt {17} & 4 / \sqrt {17}
    \end{bmatrix}\\
    & = \begin{bmatrix}
      1\\
      2
    \end{bmatrix}
    \begin{bmatrix}
      1 & 4
    \end{bmatrix}
  \end{aligned}
\]

$u_1$ is a basis for the column space, $u_2$ a basis for the row space kernel, $v_1$ a basis for the row space, $v_2$ a basis for the kernel.

## 3. 

Fibonacci matrix:

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      1 & 1\\
      1 & 0
    \end{bmatrix}
  \end{aligned}
\]

I won't do this one by hand.

```{r, results='asis'}
A <- square(1, 1, 1, 0)
AtA <- t(A) %*% A
V <- eigen(AtA)$vectors
Sigma <- diag(x = sqrt(eigen(AtA)$values))
U <-   A %*% V %*% (diag(x = 1 / diag(Sigma)))

mat2latex(U %*% Sigma %*% t(V))
```

## 5.

Alternate approach: find both matrices' vectors by hand

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      1 & 1 & 0\\
      0 & 1 & 1
    \end{bmatrix}\\
    & A^TA = \begin{bmatrix}
      1 & 1 & 0\\
      1 & 2 & 1\\
      0 & 1 & 1
    \end{bmatrix}\\
    & V = \begin{bmatrix}
      1/ \sqrt 6 & 1/ \sqrt 2\\
      2 / \sqrt 6 & 0\\
      -1 / \sqrt 6 & -1 / \sqrt 2
    \end{bmatrix}
  \end{aligned}
\]

with relevant eigenvalues of $(3, 1)$.

Then \[U =  1 / \sqrt 2\begin{bmatrix}
  1 & 1\\
  1 & -1
\end{bmatrix}\]

The big decomposition:

\[A =  \frac{1}{\sqrt 2} \begin{bmatrix}
  1 & 1 & 0\\
  1 & -1 & 0
\end{bmatrix} \begin{bmatrix}
  \sqrt 3 & 0 & 0\\
  0 & 1 & 0\\
  0 & 0 & 0
\end{bmatrix} \begin{bmatrix}
  1/ \sqrt 6 & 2/ \sqrt 6 & -1 / \sqrt 6\\
  1 / \sqrt 2 & 0 & -1 / \sqrt 2\\
  0 & 0 & 0
\end{bmatrix}  \]

## 8.

$A^TA$ is a diagonal matrix of $\sigma_1^2, \dots, \sigma_n^2$, so $V^T$ is just $I_m$. $U$ is a diagonal matrix of $Av_1 / \sigma, \dots, Av_n / \Sigma_n$. Summing up:

\[A = AV \Sigma^+ \begin{bmatrix}
  \sigma_1 &  & \\
   & \ddots & \\
   &  & \sigma_m
\end{bmatrix} I_m \]

##9.

The formulation $A = \sigma_1u_1v^T + \dots + \sigma_ru_rv_r$ breaks up$A$ into a series of matrices representing transformations of of the eigenspaces of $A$'s row space into its column space. We need $r$ terms because $A$'s rank is the number of elements in its basis.

## 10.

From this information, the singular values are just the eigenvalues themselves
($A$ is square). Since it is also symmetric, $U$ is just the eigenvectors and $V^T= U^T$. Then:

\[A = \begin{bmatrix}
  u_1 & u_2
\end{bmatrix} \begin{bmatrix}
  3 & 0\\
  0 & -2
\end{bmatrix} \begin{bmatrix}
  u_1 & u_2
\end{bmatrix}^T \]

## 12.

a. If $A =4A$ then $A^TA = 16A^TA$, so the singular values are increased by that factor. The unit eigenvectors are unaffected.

b. For $A^T$

\[\begin{aligned}
&A^T = (U \Sigma V^T)^T &A^{-1} = (U\Sigma V^T)^{-1}\\
& = V (U \Sigma)^T &= (V^T)^{-1} (U \Sigma)^{-1}\\
& = V \Sigma U^T &= V \Sigma^{-1} U^T
\end{aligned}\]
Naturally, the inverse only exists if $A^{-1}$ does, which
requires a square matrix.

## 11

Add $-\sigma_1I$ to $A$ to get a singularity.

## 13.

If $A = A+I$, then $A^TA = (A+I)^T(A+I) = A^2 + A^T + A + I$, which implies $\Sigma = \Sigma^2 + 2 \Sigma + I$

## 14.

The SVD of the zero matrix is just $I_n 0{n \times m} I_m$.
Its pseudoinverse is the $m \times n$ zero matrix, not that that's very useful.

## 15.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      1 & 1 & 1 & 1
    \end{bmatrix}\\
    & A = \begin{bmatrix}
      1
    \end{bmatrix}
     \begin{bmatrix}
      2 & 0 & 0 &0
    \end{bmatrix}
    \begin{bmatrix}
      1/2 & 1/2 & 1/2 & 1/2\\
      0 & 1 & 0 & 0\\
      0 & 0 & 1& 0\\
      0& 0 & 0 & 1
    \end{bmatrix}\\
    & A^+ = \begin{bmatrix}
      1/2 & 0 & 0 &0\\
      1/2 & 1 & 0 & 0\\
      1/2 & 0 & 1 &0\\
      1/2 & 0 & 0 & 1
    \end{bmatrix} 
    \begin{bmatrix}
      1/2\\
      0\\
      0\\
      0
    \end{bmatrix} 
    \begin{bmatrix}
      1
    \end{bmatrix} = \begin{bmatrix}
      1/4\\
      1/4\\
      1/4\\
      1/4
    \end{bmatrix}
  \end{aligned}
\]

b.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      0 & 1 & 0\\
      1 & 0 & 0
    \end{bmatrix}\\
    & A = \begin{bmatrix}
      0 & 1\\
      1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 &0\\
      0 & 1 & 0\\
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 &1
    \end{bmatrix}\\
    & A^+ =  \begin{bmatrix}
      0 & 1\\
      1 & 0
    \end{bmatrix}
  \end{aligned}
\]

c.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      1 & 1\\
      0 & 0
    \end{bmatrix} \\
    & A = \begin{bmatrix}
      1 & 0\\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      \sqrt 2 &0\\
      0 & 0
    \end{bmatrix} 
     \begin{bmatrix}
      1 / \sqrt 2 & 1/ \sqrt 2\\
       1/ \sqrt 2 & -1 /\sqrt 2
    \end{bmatrix}\\
    & A^+ = \begin{bmatrix}
      1/2 & 0\\
      1/2 & 0
    \end{bmatrix}
  \end{aligned}
\]

## 16.

If $m \times n$ $Q$ has orthonormal columns, then $\Sigma = I_{n \times m}$, so the pseudoinverse is $VU^T$.

## 17.

\[A = \frac{1}{\sqrt {10}} \begin{bmatrix}
  10 & 6\\
  0 & 8
\end{bmatrix}\]

Diagonalize, find positive definite square root, then polar decomposition:

\[
  \begin{aligned}
    & A^TA =\begin{bmatrix}
      10 & 6\\
      6 & 10
    \end{bmatrix}\\
    & V = \begin{bmatrix}
      1 & 1\\
      1 & -1
    \end{bmatrix}\\
    & \sqrt \Sigma = \begin{bmatrix}
      4 & 0\\
      0 & 2
    \end{bmatrix}\\
    & V \sqrt \Sigma V^T = \begin{bmatrix}
      3 & 1\\
      1 & 3
    \end{bmatrix} 
  \end{aligned}
\]

We now need to find $Q = UV^T$. Since we have $A$ and $U = V \Sigma$:

\[
  \begin{aligned}
    & U = \frac{1}{ \sqrt 20} \begin{bmatrix}
      10 & 6\\
      0 & 8
    \end{bmatrix} \begin{bmatrix}
      1 & 1\\
      1 & -1
    \end{bmatrix}
    \begin{bmatrix}
      1/4 & 0\\
      0 & 1/2
    \end{bmatrix}
  \end{aligned}
\]

## 18.

Use the generalized inverse for a least-squares solution.

```{r, results="asis"}
A <- square(c(rep(1, 3), 0, 0, 1, 0, 0, 1))
V <- eigen(t(A) %*% A)$vectors
Sigma <- sqrt(diag(x = eigen(t(A) %*% A)$values))
U <-  A %*% V %*% diag(x = c(1/2, 1, 0))

A_plus <-  V %*% diag(x = c(1/2, 1, 0)) %*% t(U)

b <- c(0, 2, 2)
mat2latex(A_plus %*% b)
```

## 18+

If $A$ has full rank in one dimension such that its generalized inverse is the left or 
right inverse, then $A^+b$ is always in the row space, since both $A^T$ and $(A^TA)^{-1}$are the leftmost matrices in either form, and the image of each is the row space.

Showing $A^TA x^+ = A^Tb$:

\[
  \begin{aligned}
    & A^Tb = A^TA(A^TA)^{-1}A^Tb\\
    &A^Tb = A^Tb\\
    & A^Tb= A^TAA^T(AA^T)^{-1}b\\
    & A^Tb = A^Tb
  \end{aligned}
\]

## 19.

\[
  \begin{aligned}
    & A = U \Sigma V^T\\
    & A = (U \Sigma U)(U^TV)\\
    & = (V^TU)(U^T\Sigma^+ U)
  \end{aligned}
\]

## 20.

$(AB)^+ \neq B^+A^+$ in general. If

\[\begin{aligned}
A = \begin{bmatrix}
  0 & 0\\
  1 & 0
\end{bmatrix} & 
B = \begin{bmatrix}
  0 & 0\\
  0 & 1
\end{bmatrix}
\end{aligned}\]

## 21.

\[
  \begin{aligned}
    & A^+ = U^T(UU^T)^{-1}(L^TL)^{-1}L^T\\
    & A^TAb = U^TL^TLUU^T(UU^T)^{-1}(L^TL)^{-1}L^Tb\\
    & = U^TL^TL(L^TL)^{-1}L^Tb\\
    & = U^TL^Tb\\
    & = A^Tb
  \end{aligned}
\]

## s22.
$AA^+$ projects onto $A$'s row space, $A^+A$ onto its image.
In either case only the first $r$ vectors of $U$ and $V$ (well, first $r$ rows of $V^T$) are selected; those $r$ vectors provide bases for the image and row space, respectively.

\[
  \begin{aligned}
    & AA^+ = U \Sigma V^TV \Sigma ^+ U^T\\
    & = U \Sigma \Sigma ^+ U^T
  \end{aligned}
\]

\[
  \begin{aligned}
    & A^+A  = V \Sigma^+ U^T U \Sigma V^T\\
    & = V \Sigma^+ \Sigma V^T
  \end{aligned}
\]

If $A$ has full column rank, $A^+A$ reduces it to $I_n$; if it has full row rank, $A^+A$ reduces it to $I_m$. If not, the middle term
selects the first $r$ rows of $U^T$ or $V^T$, yielding the projection. This matrix is idempotent and therefore a projection:

\[
  \begin{aligned}
    & A^+A = V \Sigma^+ \Sigma V^T\\
    & (A^+A)^2 = V \Sigma^+\Sigma V^T V \Sigma^+\Sigma V^T\\
    & = V \Sigma^+ \Sigma  \Sigma^+ \Sigma V^T\\
    & = V \Sigma^+ \Sigma V^T
  \end{aligned}
\]

If $A$ lacks full row or column rank, some singular values are 0,
which is why we know it's a projection, not a rotation. If not, the product is the identity, also a projection.