---
title: "Section 3.3 Problems"
author: "Ryan Heslin"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.pos = "", warning = FALSE,
                      tidy = TRUE, fig.align = "center")

library(tidyverse)
```

## 1.

The least squares solution to

\[
  \begin{aligned}
    & 3x = 10\\
    & 4x = 5
  \end{aligned}
\]

is 

```{r}
fit <- function(A){
  (solve(t(A) %*%A) %*% t(A))
}

fitted <- c(3, 4) %*% fit(c(3, 4)) %*% c(10, 5)
t((c(10, 5) - fitted)) %*% c(3, 4)
```

I check the solution is orthogonal, just to be safe.

## 2.

$D = 3$.

## 3.

The equation is:

\[
  E^2 = (u-1)^2 + (v-3)^2 + (u+v - -4)^2    
\]
So: 

\[
    \begin{aligned}
    & \frac{\partial f}{\partial u} = 2(v-3) + 2(u + v -4)\\ 
    & = 2v + u -7 \\
    & \frac{\partial f}{\partial u} = 2(u -1) + 2(u + v - 4)\\ 
    & = 2u + v - 5
    \end{aligned}
\]

Then just solve those two equations to get $(1, 3)$.

```{r}
fit(rbind(diag(nrow = 2), c(1, 1))) %*% c(1, 3, 4)
```
$b$ happens to lie within the span of $A$ here.

## 6.

```{r}
A <- matrix(c(1, 1, -2, 1,-1, 4), nrow = 3)
parallel <- A  %*% fit(A) %*% c(1, 2, 7)
c(1, 2, 7) - parallel
```

## 7.

Find a projection matrix:

```{r}
A <- cbind(c(1, 1, -2), c(1, -1, 4))
A %*% solve(t(A) %*% A) %*% t(A)
```

## 8.

If $P$ projects onto the $k$-dimensional subspace, then $P$'s image is $k$, and its rank is the dimension of $k$.

## 9.

IF $P=P^TP$, then $P$ is a projection matrix. Projection matrices are both symmetric and respect unit length, so $P^TP=P^2=P$.

b. $P=0$ projects into the kernel of the transpose, since $0_n$ (from the right-hand matrix) resides there.

## 10.

Say $v$, $w$, and $b$ are orthogonal, then $A^TA=I_m$ and $A^Tb$ maps $b$ onto $A$'s column space.

## 11.

Say $P$ projects onto $S$ and $Q$ onto $S^{\perp}$. Then $P + Q = I$ because every vector consists of $Px + Qx$, so $(P + Q)x =x$ $PQ =0$ because $Px \cdot Qx = 0$. Then

\[
  \begin{aligned}
    & (P - Q)^2 = I\\
    & P^2 - QP - PQ + Q^2 =I\\
    & P + Q = I\\
    & I = I
  \end{aligned}
\]

## 12.

The kernel of the transpose is 

\[
  \begin{bmatrix}
  -1 & -1\\
  1 & 0\\
  0 & 0\\
  0 &1
  \end{bmatrix}
\]

```{r}
A <- cbind(c(-1, 1, 0, 0), c(-1, 0, 0, 1))
A %*% fit(A)
```
Since all vectors in $V$ and $V^{\perp}$ are orthogonal, the projection of a vector in one onto the other is 0.

   
## 14.

One such vector is

\[
     \begin{bmatrix}
     1\\
     -1\\
     0
     \end{bmatrix}
\]

## 15.

Show the reflection matrix $R$ is involutory:

\[
  \begin{aligned}
    & R^2 = (I - 2P)^2\\
    & = I^2 +4P^2 -2PI - 2IP + I^2\\
    & = I^2\\
    & = I
  \end{aligned}
\]

## 16.

Show $P = uu^T$.

Symmetry is obvious. For idempotence, consider the first element of $P^2$:

\[
  \begin{aligned}
    & = (u_1^2)^2 + (u_1+u_2)^2\\
    & = u_1^2(u_1^2 +u_2^2)\\
    & = u_1^2
  \end{aligned}
\]

## 17.

That matrix is $`r tcrossprod(1/sqrt(2) * c(-1, 1))`$

## 18.

I wind up with the system:

\[
    \begin{aligned}
    & 4c -+ 3t + 5z = 14\\
    & 2c + 3t + 2z = 8\\
    & 3c +3t + 3z = 14
    \end{aligned}
\]

## 19.

The row space projection is of course $A^T(AA^T)^{-1}A$


## 20.

Since they are orthogonal complements,  $I - A(A^TA)^{-1}A^T$

## 21.

## 22.

Algebra gives a solution of $x_1 = 2, x_2 = -1$, yielding points $(2, 2, 0)$ and $2, 0, 4$.

## 23.

The best fit to a constant function is the average. Then $||\hat x - x|| =||\bar x - x|| = \sqrt{\bar x - x}$, which is the exact definition of the residual sum of squares.

## 24.

Quadratic fit:

```{r}
A <- matrix(c(1, -1, 1,
            1, 0, 0,
            1, 1, 1,
            1, 2, 4), nrow = 4)
fit(A) %*% c(2, 0, -3, -5)
```
The equation is $1 -5t$; the coefficient on $t^2$ is barely significant.
  

## 27

### a.

\[
    \begin{aligned}
    & a^Ta\hat x = a^T b\\ 
    & n \hat x = \sum b \\ 
    & x = \frac {\sum b}{n}\\ 
    & x = \bar b
    \end{aligned}
\]

### b.

The error is the centered vector, the variance $(b - a \hat x) (b - a \hat x)^T$, the standard deviation the square root of this.

### c. 

All works as it should.

```{r}
a <- c(1, 2, 6)
 fitted <- a %*% fit(a) %*% c(1,2, 6)
crossprod(a -fitted, c(1, 2, 6) )
```

## 28.

\[
    \begin{aligned}
    & A^TA^{-1}A^T(b - Ax)\\ 
    & A^TA^{-1}A^Tb - A^TA^{-1}A^TAx\\ 
    & x - x\\ 
    & 0
    \end{aligned}
\]

## 30.

$(b - \bar b)(b - \bar b)^T / 4$

## 31. 

9/10.

## 37.

A proof: 

\[
    \begin{aligned}
    & A^TAx = A^Tb \\ 
    & n x_1 + x_2 \sum_{i=1}^{n}{t} = \sum_{i=1}^{n}{b} \\ 
    & x_1 + x_2 \hat t = \hat b \\ 
    & x_1 = \hat b - x_2 \hat t
    \end{aligned}
\]

## 38.

\[
    \begin{aligned}
    & \hat x_w = \frac {w_1^2b_1 + w_2^2b_2}{w_1^2 +w_2^2} \\ 
    & = \frac {w_2^2b_2}{w_2^2}\\ 
    & = b_2
    \end{aligned}
\]

## 39.

\[
    \frac { \sum_{i=1}^{m}{w_i^2 b_i} }{ \sum_{i=1}^{m}{w_i^2}}
\]

## 40.

Respectively 11 and 5. The perpendicular line is given by $(1, -4)$.

## 41.

Weighted least squares! 


```{r}
W <- diag(x = c(2, 1, 0))
A <- cbind(1, c(0, 1, 2))
b <- c(0, 1, 1)
x_w <- solve(t(A) %*% W^2 %*% A) %*% t(A) %*% W^2 %*% b 
fitted <- A %*% x_w
t(b-fitted) %*% W^2 %*% b
```

## 42. 

Since the expectation is 0, just square to get the variance.
```{r}
e <- c(-2, -1, 5)
probs <- c(.5, .25, .25)
crossprod(e, probs)
crossprod(e, e) /3 
e_2 <- c(-1, 0, 1)
crossprod(e_2, e_2) / 3
```
The inverses of the variances, $(1/10, 3/2)$
