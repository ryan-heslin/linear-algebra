---
title: "`r params$title`"
author: "Ryan Heslin"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
header-includes:
  - \setlength{\parindent}{2em}
  - \setlength{\parskip}{2em}
params:
    title: "Section 7.2 Problems"
output:
  pdf_document:
    highlight: "kate"
    df_print: "kable"
    includes:
        "notes_text_preamble.tex"
---

```{r include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    comment = "",
    fig.pos = "",
    message = FALSE,
    tidy = "styler",
    warning = FALSE,
    fig.align = "center",
    highlight = TRUE
)
```


# 1.

# 2.

# 3.

# 4.

# 5.

# 6.

# 7.

# 8.

# 9.

# 10.

# 11.

# 12.

(-1, 0, 1, 1).

# 13.

# 14.

Any eigenvalue of a block on the diagonal is also an eigenvalue of $A$, because 
subtracting the eigenvalue will make the whole matrix singular.

# 15.

\[
    \begin{aligned}
    & \lambda^2 -2 \lambda + 1 -k\\
    & \frac {2 \pm sqrt{4 - 4(1)(1-k)}}{2}
    & 1 \pm \frac {4k}{2}
    & 1 \pm k
    \end{aligned}
\]
If $k = 0$ the eigenvalue 1 is repeated; if $k< 0$ they are both imaginary; 
otherwise, both are real.

# 16.

The quadratic formula ultimately leads to 

\[
    \frac{a +c \pm 2 \sqrt{ 1^ + 4b^2 +c^2 -2ac}}{2}
\]
If $a = -c$, then 

\[
    \begin{aligned}
    & a^2 +4b^2 +a^2 2a^2 =0\\
    & 4a^2 +4b^2 = 0 
    & a = b = 0
    \end{aligned}
\]
so repreat eigenvalues 
are possible only if both are zero. I'd rather not find out the imagniary and real cases.

# 17.

They work out to $\sqrt(a^2 +b^2)$. 
I think this is a rotation-scaling $\frac{2 \pi}{2}$ left.

# 18.

By computing the determinant of 

\[
\begin{bmatrix}
a - \lambda & b\\
b & a - \lambda
\end{bmatrix}
\]
we find
\[
    \begin{aligned}
    & (a - \lambda^2)\\
    & a^2 -2a \lambda + \lambda^2 -b^2 \\ 
    & \frac {2a \pm \sqrt{(-2a)^2 -4(1)(a^2 -b^2)}}{2}\\
    & \frac {2a \pm \sqrt{4b^2}}{2}\\
    & a \pm b
    \end{aligned}
\]

# 19.

True. Since $\det  A = \lambda_1 \lambda_2$, it can only be negative if the eigenvalues are opposite-signed, which means they must be distinct.  


# 20.


By the definition of an eigenvalue, $\dim(\ker(A - \lambda I)) \geq 1$ (because this 
matrix has zero determinant, so its kernel has nonzero dimension). So each 
eigenvector has geometric multiplicity of 1. The eigenvectors 
also must be distinct if the eigenvalues are. If they shared an eigenvector $v$, then 
$\lambda_1 v = \lambda_2 v$, because $\lambda_1 v = Av$ and $\lambda_2 v = Av$, which 
is impossible if $\lambda_1 \neq \lambda_2$. With two distinct eigenvectors, $A$ 
can be diagonalized $A = S \Lambda S^{-1}$.

# 21.

# 22.

They are identical because $tr(A) = tr(A^T)$ and $\det A = \det A^T$. They also 
therefore have the same eigenvalues.

# 23.

If they are similar, then $B =S^{-1}AS$. Then if $A$ can be diagonalized $A = T \Lambda T^{-1}$, 
$B = A$ if $S = TX$ for some other matrix $X$.(I think this holds for the Jordan diagonalization 
as well if $A$ can't be diagonalized). So these two matrices have the same eigenvalues 
and thus the same characteristic polynomials, but different eigenvectors.


# 24.

The polynomial is $\frac{1}{4}(\lambda -1)(\lambda - 4)$, so 1/4 and 1. 

# 25.

By substituting into $Av = \lambda v$, we get 

By solving 

\[
    0 =\begin{bmatrix}
    a- \lambda & b\\
    c & d - \lambda
    \end{bmatrix} 
    v
\]

for the two eigenvectors, we get>


\[
    a - \lambda -b = 0 \implies \lambda = 1 -b
\]
for $\begin{bmatrix}1\\-1 \end{bmatrix}$, and 

\[
    b(a- \lambda) = -bc \implies \lambda = a +c = 1
\]

for $\begin{bmatrix}b\\c \end{bmatrix}$

The first eigenvalue has an absolute value less than 1 because $a \leq 1$ and $b \geq 0$  






# 26.

# 27.

We know the eigenvalues, so we just have to 
find the eigenvectors, diagonalize, and multiply out. That comes to: 

\[
\begin{bmatrix}
1 & 0\\
0 & -(1/4)^k
\end{bmatrix}
\]

The matrix approaches its steady state. The 
eigenvector associated with the dominant eigenvalue stays in place ( $A^k v = \lambda^k v = v$ for $\lambda = 1$ ), and 
the one associated with the eigenvalue less than 1 in absolute value dies.
```{r}
`%^%` <- function(lhs, rhs) {
    out <- lhs
    for (i in seq_len(rhs)) {
        out <- out %*% lhs
    }
    out
}
A <- matrix(c(.5, .5, .25, .75), nrow = 2)
A %^% 100
A %^% 100 %*% c(1, -1)
A %^% 100 %*% c(1, 2)
```

# 28.


```{r}
A <- matrix(c(.8, .2, .1, .9), nrow = 2)
A %^% 100
```

## b. 

The full formula is 


\[
    w(0)\begin{bmatrix}
  1 + 2(7/10)^t\\
    2 - 2(7/10)^t
    \end{bmatrix} +
    m(0)\begin{bmatrix}
    1 - (7/10)^t\\
    2 + (7/10)^t
    \end{bmatrix}
\]

## c.
 
Never, because the steady state has 1/3 of the townspeople at the Wipfs', which 
is 400.



# 29.

It always has eigenvalue 1 because $A1$ (if 1 designates a vector of 1s) is 1 (since the rows sum to 1). Therefore $A1 = \lambda 1 = 1$, so $\lambda = 1$.   

# 30.

### a. 


Let $v$ be some eigenvector with a non-1 eigenvalue, $k$ a constant vector 
of $v$'s largest element, and $u := k -v$. Then $Av = A(k-u) = k- Au = k - \lambda_u A$. 
But if $\lambda_v > 1$, then $v$'s largest element will become greater than $k$ by 
a factor of $\lambda_v$ , but 
this is impossible. $k\sum_{i=1}^{n}{a_i} = k$ because $\sum_{i=1}^{n}{a_i} = 1$ 
for all rows of $a$, yet every element of $v$ is less than or equal to $k$. It 
cannot be 1 either because that eigenvalue already belongs to $k$, nor can it be 
negative because all elements of both $A$ and $v$ are positive or zero, so 
$0 \le \lambda_v < 1$ 

If $k$ is the largest element of $v$, then $a_{iv}$ (where $a_i$ is a row of $a$ )
is less than or equal to $\sum_{i=1}^{n}{a_{ij}k_i}$ (the dot product of a vector whose elements 
are all $k$, since for at least one $v_i$ $v_i < k$). ($Av_{i} > k$ only if $k$ is not the largest element of $v$, and remember $A$ is all positive ). The other 
elements are less than or equal to it for the same reasoning. So at least one element 
of $Av$ is less than the corresponding element of $Ak$, which means the eigenvalue 
of $v$ must be less than that of $k$.
For any vector of constants $c$, $a_ic= c \sum_{i=1;j=1}^{n}{a_{ij}}=c$, meaning the constant 
vector is an eigenvector with eigenvalue 1. Therefore, for any non-constant positive 
eigenvector, the eigenvalue is less than 1, but must be positive or 0 because 
all elements of $A$ are positive or 0, so $Av$ can have no negative elements. 

### b. 

If some elements of $v$ are negative, then $a_ i v  \le  a_i u $ for an 
eigenvector $u$ with the absolute values of $v$ , because all elements of $A$ are positive. 
For the reasons outlined above, the eigenvalue of $u$ is positive but less than 
or equal to 1, so the eigenvalue of $v$ must lie in $(-1, 1)$, as shown below.  

### c. 

The only vector that could have an eigenvalue that great in absolute 
value is a vector of negative 1, because $Ak = k$ since $A$ is all positive, but its eigenvalue is 1 because it is a 
multiple of the eigenvector described above, satisfying $Ak = k$ because all elements are 
negative. For the reasons outlined above, 
any all-negative eigenvector, if all its elements were replaced the negative sign of its largest in absolute value, 
would have eigenvalue 1. Constant vectors have eigenvalue 1 
because $a_iv = \sum_{i=1;j=1}^{n}{a_{ij}c_i} = c \sum_{i=1}^{n}{a_i} = c(1) = c$ 

# 31.

1 must be an eigenvalue because if $A$ is a 
transition matrix, $A^T$ must have rows that sum to $1$, which implies the existence of such 
an eigenvector. Other eigenvalues are less than or equal to 1 in absolute value for the reasons shown above. But the eigenvector is not necessarily the same, since the eigenvectors of $A$ are not generally those of $A^T$. It is guaranteed 
to be an eigenvector if $A$ is symmetric, of course.

# 32.


They are $\pm 2$. 
```{r}
library(ggplot2)
polynomial <- function(lambda) lambda^3 - 3 * lambda
X <- data.frame(x = seq(-10, 10))
ggplot(X, aes(x = x)) +
    geom_function(fun = polynomial) +
    lims(x = c(-10, 10), y = c(-10, 10))

optimize(polynomial, interval = c(-5, 5))
optimize(polynomial, maximum = TRUE, interval = c(-5, 5))
```

# 33.

### a. 

\[
    \begin{aligned}
    & -\lambda(-\lambda(c - \lambda) - b) -1(-a) + 0\\
    & -\lambda(-\lambdac + \lambda^2 -b) + a
    & -\lambda^3 + \lambda^2 c + \lambda b + a
    \end{aligned}
\]

### b. 

Just take the hint: 


\[
\begin{bmatrix}
0 & 1 & 0\\
0 & 0 & 1\\
\pi & -5 & 17 
\end{bmatrix}
\]



# 34.

They may both have algebraic multiplicity 2. Either may have algebraic multiplicity 2 and the other 3. Or both may have algebraic multiplicity 1, with the remaining two eigenvalues a complex conjugate pair.


# 35.

Here is one. 
```{r}
theta <- pi / 3

A <- matrix(c(
    cos(theta), sin(theta), 0, 0,
    -sin(theta), cos(theta), 0, 0,
    0, 0, cos(theta), sin(theta),
    0, 0, -sin(theta), cos(theta)
),
nrow = 4
)

eigen(A)
```

# 36.

Put $n$ $2 \times 2$  block rotation matrices of an angle like $\pi/3$ on the diagonal. 
# 37.

If all eigenvalues are distinct, the 
derivative of the characteristic polynomial evaluated at $\lambda_0$, by the Product Rule, is $f\prime(\lambda_0) = -1(\lambda_1 - \lambda)\dots(\lambda_n - \lambda) + (\lambda_0 - \lambda_1)( \frac {d}{d \lambda}[(\lambda_1 - \lambda)\dots \lambda_n - \lambda])$, which is not generally 0, since the $f\prime(x) g(x)$ part is not necessarily 0 . But if $\lambda_0$ is a 
repeat eigenvalue, then: 
\[
    \begin{aligned}
    & f\prime(\lambda_0) = -n(\lambda_0 -\lambda)^{n+1}(\lambda_1 - \lambda) \dots (\lambda_n - \lambda) + (\lambda_0 - \lambda)^n(\lambda_1 -\lambda) \dots (\lambda_n - \lambda)\\
    & = 0 + 0 \\ 
    & = 0
    \end{aligned}
\]



# 38.

By algebra, 7 and -2. 


# 39.

For $n \times n$ matrices, element $ii$ of 
the trace of $AB$ is $a^T_i b_i$ (where the subscript denotest the column). For $BA, it is $b^T_i a_i$, which is equivalent.

# 40.

As shown above. 

# 41.

If they are similar, they have the same 
eigenvalues, so the sums of eigenvalues, and 
therefore the traces, are equal.

# 42.

If $BA = 0$, then $tr(BA) =0$, and we just showed that $tr(AB) = tr(BA)$. Therefore: 

\[  \begin{aligned}
    & tr((A+B)^2) = tr(A^2) + tr(B^2) \\
    & = tr(A^2) + tr(B^2) + AB + BA\\ 
    & =  tr(A^2) + tr(B^2
    \end{aligned}
\]

# 43.

No, because since $tr(AB) = tr(BA)$, tr(AB - BA) = tr(BA - AB) = 0$.

# 44.

The condition implies
\[
    \begin{aligned}
    & AB -BA = I\\
    & AB = BA + I
    & B = A^{-1}BA + A^{-1}
    \end{aligned}
\]


# 45.

The eigenvalue satisfies (we can ignore the negaitve branch, since it makes no impact):

\[
    \begin{aligned}
    & 1 \pm 2 \sqrt{1 +k} = 5\\
    & 2 \sqrt{1 +k} = 4 \\ 
    & \sqrt{1 + k} = 2\\ 
    & 1 + k = 4 \\ 
    & k = 3
    \end{aligned}
\]


# 46.

### a. 

\[
    \begin{aligned}
    & (\lambda_1 + \lambda_2)^2 = (a+d)^2\\
    & \lambda_1^2 + \lambda_2^2 + 2 \lambda_1 \lambda_2 = a^2 + 2ad + d^2 \\ 
    & \lambda_1^2 + \lambda_2 ^2 + 2(ad -bc) = a^2 + 2ad + d^2 \\ 
    & \lambda_1^2 + \lambda_2^2 = a^2 + d^2 + 2bc
    \end{aligned}
\]

###.b 

Subtract $a^2 +d^2$ from both sides to get $2bc \leq b^2 + c^2$, an old algebraic identity that provides the basis for Cauchy-Swarz. (I had to look it up, but of course the sum of squares has to be greater than the cross term to be positive definite in cases where one term is subtracted from the other). 

### c. 

Diagonal matrices, for which $b = c = 0$ 

# 47.

Those for which nonzero columns of $M$ satisfy $Am = \lambda m$ in other words, 
those matrices $A$ for which nonzero columns of $M$ correspond to the eigenvalues 
of $D$. The requirement that $M$ be nonzero means this can work for matrices that 
have at least one of the eigenvalues of $D$, for example: 

\[
\begin{bmatrix}
2 & 0\\
0 & 4
\end{bmatrix}
\begin{bmatrix}
1 & 0\\
0 & 0
\end{bmatrix} = 
\begin{bmatrix}
1 & 0\\
0 & 0
\end{bmatrix}
\begin{bmatrix}
2 & 0\\
0 & 3
\end{bmatrix}

\]



# 48.

Those similar to $D$, because given $S$ is invertible $A = S D S^{-1}$, which 
is the standard diagonalization of $A$ .

# 49.

The same as for the $2 \times 2$ case.

# 50.

### a. 

It is easy to see by inspection that $x = 2$ is the real solution. 

### b. 

By substitution: 

\[
    \begin{aligned}
    & (v-u)^3 + 6(v -u) = 20\\
    & (v^2 -2uv + u^2)(v -u) + 6v -6u = 20 \\ 
    & u^3 - uv^2 - 2v^2u + 2u^2v  +u^2v -u^3  +6v -6u = 20 \\ 
    & 20 - 6v + 6u +6v - 6u = 20 \\ 
    & 20 = 20
    \end{aligned}
\]

### c.

This takes some algebra: 

From $uv =2$, $u = 2/v$ and $v = 2/u$. 
So: 

\[
    \begin{aligned}
    & v^3 -u^3 = uv\\ 
    & 2(v -u) = 20 \\ 
    & v - u = 10 \\ 
    & v = u + 10
    \end{aligned}
\]

substituting into the second equation: 

\[
    \begin{aligned}
    & v(v-10) = 2\\
    & v^2 -10v + -2 = 0
    & \frac {10 \pm \sqrt{-10^2 -4(1)(-2)}}{2(1)}
    & v = 5 \pm 3 \sqrt{3}
    \end{aligned}
\]


because $\sqrt{108} = \sqrt{9}\sqrt{12} = 6 \sqrt{3}$, then divide by the denominator 

It turns out $5 - 3 \sqrt 3$ satisfies both $v = u +10$ and $v = 2/u$, so it 
is the solution.

By subsitution: 

\[
    \begin{aligned}
    & 8/u^3 - u^3 = 20\\
    & u^6 + 20 u^3 -8 = 0 
    & t^2 + 20 t -8 = 0 
    & t = -10 \pm 6 \sqrt 3
    & u = (-10 \pm 6 \sqrt 3)^{1/3} 
    & v = 2/(-10 \pm 6 \sqrt 3)^{1/3} 
    \end{aligned}
\]

Either solution satisfies both equations



