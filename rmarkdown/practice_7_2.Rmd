---
title: "`r params$title`"
author: "Ryan Heslin"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
header-includes:
  - \setlength{\parindent}{2em}
  - \setlength{\parskip}{2em}
params:
    title: "Notes"
output:
  pdf_document:
    highlight: "kate"
    df_print: "kable"
    includes:
      in_header: "notes_text_preamble.tex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "",
  fig.pos = "",
  message = FALSE,
  tidy = "styler",
  warning = FALSE,
  fig.align = "center",
  highlight = TRUE
)
```


# 1.

# 2.

# 3.

# 4.

# 5.

# 6.

# 7.

# 8.

# 9.

# 10.

# 11.

# 12.

(-1, 0, 1, 1).

# 13.

# 14.

Any eigenvalue of a block on the diagonal is also an eigenvalue of $A$, because 
subtracting the eigenvalue will make the whole matrix singular.

# 15.

\[
    \begin{aligned}
    & \lambda^2 -2 \lambda + 1 -k\\
    & \frac {2 \pm sqrt{4 - 4(1)(1-k)}}{2}
    & 1 \pm \frac {4k}{2}
    & 1 \pm k
    \end{aligned}
\]
If $k = 0$ the eigenvalue 1 is repeated; if $k< 0$ they are both imaginary; 
otherwise, both are real.

# 16.

The quadratic formula ultimately leads to 

\[
    \frac{a +c \pm 2 \sqrt{ 1^ + 4b^2 +c^2 -2ac}}{2}
\]
If $a = -c$, then 

\[
    \begin{aligned}
    & a^2 +4b^2 +a^2 2a^2 =0\\
    & 4a^2 +4b^2 = 0 
    & a = b = 0
    \end{aligned}
\]
so repreat eigenvalues 
are possible only if both are zero. I'd rather not find out the imagniary and real cases.

# 17.

They work out to $\sqrt(a^2 +b^2)$. 
I think this is a rotation-scaling $\frac{2 \pi}{2}$ left.

# 18.

By computing the determinant of 

\[
\begin{bmatrix}
a - \lambda & b\\
b & a - \lambda
\end{bmatrix}
\]
we find
\[
    \begin{aligned}
    & (a - \lambda^2)\\
    & a^2 -2a \lambda + \lambda^2 -b^2 \\ 
    & \frac {2a \pm \sqrt{(-2a)^2 -4(1)(a^2 -b^2)}}{2}\\
    & \frac {2a \pm \sqrt{4b^2}}{2}\\
    & a \pm b
    \end{aligned}
\]

# 19.

True. Since $\det  A = \lambda_1 \lambda_2$, it can only be negative if the eigenvalues are opposite-signed, which means they must be distinct.  


# 20.


By the definition of an eigenvalue, $\dim(\ker(A - \lambda I)) \geq 1$ (because this 
matrix has zero determinant, so its kernel has nonzero dimension). So each 
eigenvector has geometric multiplicity of 1. The eigenvectors 
also must be distinct if the eigenvalues are. If they shared an eigenvector $v$, then 
$\lambda_1 v = \lambda_2 v$, because $\lambda_1 v = Av$ and $\lambda_2 v = Av$, which 
is impossible if $\lambda_1 \neq \lambda_2$. With two distinct eigenvectors, $A$ 
can be diagonalized $A = S \Lambda S^{-1}$.

# 21.

# 22.

They are identical because $tr(A) = tr(A^T)$ and $\det A = \det A^T$. They also 
therefore have the same eigenvalues.

# 23.

If they are similar, then $B =S^{-1}AS$. Then if $A$ can be diagonalized $A = T \Lambda T^{-1}$, 
$B = A$ if $S = TX$ for some other matrix $X$.(I think this holds for the Jordan diagonalization 
as well if $A$ can't be diagonalized). So these two matrices have the same eigenvalues 
and thus the same characteristic polynomials, but different eigenvectors.


# 24.

The polynomial is $\frac{1}{4}(\lambda -1)(\lambda - 4)$, so 1/4 and 1. 

# 25.

By substituting into $Av = \lambda v$, we get 

By solving 

\[
    0 =\begin{bmatrix}
    a- \lambda & b\\
    c & d - \lambda
    \end{bmatrix} 
    v
\]

for the two eigenvectors, we get>


\[
    a - \lambda -b = 0 \implies \lambda = 1 -b
\]
for $\begin{bmatrix}1\\-1 \end{bmatrix}$, and 

\[
    b(a- \lambda) = -bc \implies \lambda = a +c = 1
\]

for $\begin{bmatrix}b\\c \end{bmatrix}$

The first eigenvalue has an absolute value less than 1 because $a \leq 1$ and $b \geq 0$  






# 26.

# 27.

We know the eigenvalues, so we just have to 
find the eigenvectors, diagonalize, and multiply out. That comes to: 

\[
\begin{bmatrix}
1 & 0\\
0 & -(1/4)^k
\end{bmatrix}
\]

The matrix approaches its steady state. The 
eigenvector associated with the dominant eigenvalue stays in place ( $A^k v = \lambda^k v = v$ for $\lambda = 1$ ), and 
the one associated with the eigenvalue less than 1 in absolute value dies.
```{r}
`%^%` <- function(lhs, rhs) {
  out <- lhs
  for (i in seq_len(rhs)) {
    out <- out %*% lhs
  }
  out
}
A <- matrix(c(.5, .5, .25, .75), nrow = 2)
A %^% 100
A %^% 100 %*% c(1, -1)
A %^% 100 %*% c(1, 2)
```

# 28.


```{r}
A <- matrix(c(.8, .2, .1, .9), nrow = 2)
A %^% 100
```

## b. 

The full formula is 


\[
    w(0)\begin{bmatrix}
  1 + 2(7/10)^t\\
    2 - 2(7/10)^t
    \end{bmatrix} +
    m(0)\begin{bmatrix}
    1 - (7/10)^t\\
    2 + (7/10)^t
    \end{bmatrix}
\]

## c.
 
Never, because the steady state has 1/3 of the townspeople at the Wipfs', which 
is 400.



# 29.

It always has eigenvalue 1 because $A1$ (if 1 designates a vector of 1s) is 1 (since the columns sum to 1). Therefore $A1 = \lambda 1 = 1$, so $\lambda = 1$.   

# 30.

# 31.

1 must be an eigenvalue because if $A$ is a 
transition matrix, $A^T$ must have rows that sum to $1$. As shown above, 1 is an eigenvalue, so 1 must be an eigenvalue of $A$ as well. But the eigenvector is not necessarily the same, since the eigenvectors of $A$ are not generally those of $A^T$.   

# 32.


They are $\pm 2$. 
```{r}
library(ggplot2)
polynomial <- function(lambda) lambda^3 - 3 * lambda
X <- data.frame(x = seq(-10, 10))
ggplot(X, aes(x = x)) +
  geom_function(fun = polynomial) +
  lims(x = c(-10, 10), y = c(-10, 10))

optimize(polynomial, interval = c(-5, 5))
optimize(polynomial, maximum = TRUE, interval = c(-5, 5))
```

# 33.

# 34.

# 35.

# 36.

# 37.

# 38.

By algebra, 7 and -2. 


# 39.

For $n \times n$ matrices, element $ii$ of 
the trace of $AB$ is $a^T_i b_i$ (where the subscript denotest the column). For $BA, it is $b^T_i a_i$, which is equivalent.

# 40.

As shown above. 

# 41.

If they are similar, they have the same 
eigenvalues, so the sums of eigenvalues, and 
therefore the traces, are equal.

# 42.

If $BA = 0$, then $tr(BA) =0$, and we just showed that $tr(AB) = tr(BA)$. Therefore: 

\[  \begin{aligned}
    & tr((A+B)^2) = tr(A^2) + tr(B^2) \\
    & = tr(A^2) + tr(B^2) + AB + BA\\ 
    & =  tr(A^2) + tr(B^2
    \end{aligned}
\]

# 43.

No, because since $tr(AB) = tr(BA)$, tr(AB - BA) = tr(BA - AB) = 0$.

# 44.

# 45.

The eigenvalue satisfies (we can ignore the negaitve branch, since it makes no impact):

\[
    \begin{aligned}
    & 1 \pm 2 \sqrt{1 +k} = 5\\
    & 2 \sqrt{1 +k} = 4 \\ 
    & \sqrt{1 + k} = 2\\ 
    & 1 + k = 4 \\ 
    & k = 3
    \end{aligned}
\]


# 46.

### a. 

\[
    \begin{aligned}
    & (\lambda_1 + \lambda_2)^2 = (a+d)^2\\
    & \lambda_1^2 + \lambda_2^2 + 2 \lambda_1 \lambda_2 = a^2 + 2ad + d^2 \\ 
    & \lambda_1^2 + \lambda_2 ^2 + 2(ad -bc) = a^2 + 2ad + d^2 \\ 
    & \lambda_1^2 + \lambda_2^2 = a^2 + d^2 + 2bc
    \end{aligned}
\]

###.b 

Subtract $a^2 +d^2$ from both sides to get $2bc \leq b^2 + c^2$, an old algebraic identity that provides the basis for Cauchy-Swarz. (I had to look it up, but of course the sum of squares has to be greater than the cross term to be positive definite in cases where one term is subtracted from the other). 

### c. 

Diagonal matrices, for which $b = c = 0$ 

# 47.

# 48.

# 49.

# 50.

