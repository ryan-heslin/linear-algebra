---
title: "Section 7.3 Problems"
author: "Ryan Heslin"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  pdf_document:
    highlight: "kate"
    df_print: "kable"
---

 <!-- Standard custom LaTeX commands -->
\newcommand{\abcd}{\begin{bmatrix}a&b\\
c&d\end{bmatrix}}
\newcommand{\m}[1]{\begin{bmatrix}#1\end{bmatrix}}

\newcommand{\vect}[1]{\begin{pmatrix}#1\end{pmatrix}}

\newcommand{\meq}[1]{\begin{split}#1\end{split}}

\newcommand{\bym}[1]{#1\times{m}}

\newcommand{\nby}[1]{n\times{#1}}

\newcommand{\subsp}[2]{\Bigg\{\begin{bmatrix}#1\end{bmatrix}:#2\Bigg\}}

\newcommand{\proj}[2]{\text{proj}_#1(#2)}

\newcommand{\refl}[2]{\text{refl}_#1(#2)}

\newcommand{\sumn}{\sum_{i=1}^n}

<!-- % 1: term 1 -->
<!-- % 2: subscript 1 -->
<!-- % 3: term 2 -->
<!-- % 4: subscript 2 -->
<!-- % 5. operation -->
\newcommand{\dotsn}[5]{#1_{1}#3_{1}#5{#1}_{2}#3_{2}{#5}\dots{#5}#1_{#2}#3_{#4}}

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = TRUE,
  comment = "",
  fig.pos = "",
  warning = FALSE,
  tidy = TRUE,
  fig.align = "center",
  highlight = TRUE
)

library(tidyverse)
library(rlang)
```

##1. 

Just find some eigenvectors.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      6 & 3\\
      2 & 7
    \end{bmatrix}\\
    & \lambda = (9, 4)\\
  & S = \begin{bmatrix}
    3 & -2\\
    1 & 1
  \end{bmatrix}
  \end{aligned}
\]

## 5.

None real, but $\lambda = 1 \pm i$

## 10.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      1 & -1 & 0\\
      0 & 0 & 0\\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 0 & 0\\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & -1 & 0\\
      0 & 0 & 0\\
      0 & 0 & 1
    \end{bmatrix}^{-1}
  \end{aligned}
\]

## 17.

$\lambda = (0, 0, 1, 1)$. Diagonalizable.

## 21.

```{r, results = "asis"}
S <- matador::square(1, 2, 2, 3)
Lambda <- diag(x = c(1, 2), nrow = 2)
A <- S %*% Lambda %*% solve(S)
A
```
## 22.

Just $\m{7 & 0\\ 0 & 7}$
## 23.

For $\m{1 & 1\\0 &1}$, $\lambda =1$, but since that yields $\m{0&1 \\0 &1}$, the only eigenvector is $e_1$. That makes sense, since $Ae_1$ just selects the first column, which contains only  single scalar.

## 25.

If $c$ only is 0, then  $A$ has distinct eigenvectors. If all three are 0, then $A$ has just one eigenvector for the repeat eigenvalue 0. 
## 26. 

Since $\det A = \prod^n_{i=1} \lambda_i$, if the determinant is negative but $n$ is positive, there must be an odd number of negative eigenvalues.

## 27.

$\lambda = (1, 5)$.

## 28. 

The eigenvalues are all just $k$, multiplicity $n$. They all have $e_1$ as the eigenvector.

## 29. 
Algebraic and geometric multiplicity are both $n-r$, since by rank-nullity $\ker(A)$ has dimension $n-r$.

## 30. 

Algebraic multiplicity is $n-m$

## 31. 
If an eigenbasis exist, both multiplicities sum to $n$, though geometric and algebraic multiplicity need not match for every distinct eigenvalue.

## 32.

The algebraic nultiplicities are the same, since the eigenvalues are shared. The dimension of $\ker(A - I \lambda)$ is $n - rank(A^T - \lambda I)$. So the dimension of the transposes eigenspace is the orthogonal complement of $A$'s image, and vice versa. So if $n =3$, then a two-dimensional eigenspace in $A$ corresponds to a one-dimensional eigenspace in $A^T$ and vice versa. 

#33.

\[
  \begin{aligned}
    & (B - \Lambda) = S^{-1}(A - \Lambda)S\\
    & = S^{-1}(AS -  \Lambda S)\\
    & = S^{-1}AS - S^{-1} \Lambda S\\
    & = B - \Lambda
  \end{aligned}
\]

## 34.

\[
  \begin{aligned}
    & B = S^{-1}AS\\
    & SB = AS\\
    &S(Bx) = A(Sx)
  \end{aligned}
\]
So if $Bx=0$, $Sx = 0$ as well.

b.

Invertible, so isomorphic.
\[
  \begin{aligned}
    & T(X) = Sx\\
    & T^{-1}x = S^{-1}Sx = x
  \end{aligned}
\]

c. 
Since $S$ has full rank, $Sx$ has the rank of $\ker B$, since $x$ is some linear combination of the kernel, so the dimension remains the same. Since $A$ and $B$ both have $n$ columns, if the kernels have dimension $m$ since hey both have dimension $n-m$.

## 35.

No, the traces are different.

## 37.

a. 
\[
  \begin{aligned}
    & Av \cdot w = v \cdot Aw\\
    & v^T A^Tw = v^TAw\\
    & v^T Aw = v^TAw
  \end{aligned}
\]

The proof of symmetric orthogonal eigenvectors

b.
\[
  \begin{aligned}
    & Av \cdot w = v \cdot Aw\\
    & \lambda_v v^Tw = \lambda_w v^Tw
  \end{aligned}
\]

Since $\lambda_w \neq \lambda_v$, this holds only if $v \cdot w =0$.

## 38.

Since $\det A = 1$ and $A$ is orthogonal, 1 or 3 eigenvalues are 1. Thus $Av = 1v = v$ for those eigenvectors. 
```{r}
x <- c(1, 0, 0, 1, 0)
y <- c(0,0, 1, 0, 1,0, 0)
z <- c(0, 0, 0, 0, 0)
sum(cumsum(x==0) == seq_along(x))

sum(cumsum(y==0) == seq_along(y))
sum(cumsum(z == 0) == seq_along(z))
```

## 39.

a. $n-m$ are 0, with equal geometric multiplicity because $\dim (\ker(A)) = n - m$ The remaining $m$ are distinct and have orthogonal eigenvectors because all projection matrices are symmetric.

b. Reflection matrices only have eigenvalues $\pm 1$ (this makes obvious geometric sense),
so algebraic multiplicity is greater than 1. The eigenvectors are bases of the subspaces of reflection, since these retain their position.

## 40.

$a=0$.

## 41.

All possible values.

## 42.

$b \neq 1$

## 43.

$a \neq 0$

## 44.

All values, since this is always symmetric.

## 45

Always diagonalizable.

## 46.

At least one of the constants is 0.

## 47.

$a=b=c=0$.