---
title: "Section 5.2 Problems"
author: "Ryan Heslin"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  pdf_document:
    highlight: "kate"
    df_print: "kable"
---

 <!-- Standard custom LaTeX commands -->
\newcommand{\abcd}{\begin{bmatrix}a&b\\
c&d\end{bmatrix}}
\newcommand{\m}[1]{\begin{bmatrix}#1\end{bmatrix}}

\newcommand{\vect}[1]{\begin{pmatrix}#1\end{pmatrix}}

\newcommand{\meq}[1]{\begin{split}#1\end{split}}

\newcommand{\bym}[1]{#1\times{m}}

\newcommand{\nby}[1]{n\times{#1}}

\newcommand{\subsp}[2]{\Bigg\{\begin{bmatrix}#1\end{bmatrix}:#2\Bigg\}}

\newcommand{\proj}[2]{\text{proj}_#1(#2)}

\newcommand{\refl}[2]{\text{refl}_#1(#2)}

\newcommand{\sumn}{\sum_{i=1}^n}

<!-- % 1: term 1 -->
<!-- % 2: subscript 1 -->
<!-- % 3: term 2 -->
<!-- % 4: subscript 2 -->
<!-- % 5. operation -->
\newcommand{\dotsn}[5]{#1_{1}#3_{1}#5{#1}_{2}#3_{2}{#5}\dots{#5}#1_{#2}#3_{#4}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.pos = "", warning = FALSE,
                      tidy = TRUE, fig.align = "center",
                      highlight = TRUE)

library(tidyverse)
library(rlang)
library(matador)
```

## 1.

Factor into $S \Lambda S$ 

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      2 & 1\\
      0 & 0\\
    \end{bmatrix}\\
    &\Lambda = \begin{bmatrix}
      2 & 0\\
      0 & 0\\
    \end{bmatrix}\\
    & S = \begin{bmatrix}
      1 & -1/2\\
      0 & 1\\
    \end{bmatrix}\\
    & A = \begin{bmatrix}
      1 & 1\\
      0 & -1\\
    \end{bmatrix}\begin{bmatrix}
      2 & 0\\
      0 & 0\\
    \end{bmatrix}
    \begin{bmatrix}
      1 & 1/2\\
      0 & 1\\
    \end{bmatrix}
  \end{aligned}
\]

## 2.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      3 & 2\\
      1 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0\\
      0 & 4\\
    \end{bmatrix}
    \begin{bmatrix}
      1 & -2\\
      -1 & 3\\
    \end{bmatrix}\\
    &A = \begin{bmatrix}
      -5 & -1\\
      0 & 1\\
    \end{bmatrix}
  \end{aligned}
\]

## 3.

$\lambda_1 = 3, \lambda_2 = \lambda_3=0$

The eigenbasis is 

\[\begin{bmatrix}
  -1 & -1 & 1\\
  1 & 0 & 1\\
  0 & 1 & 1\\
\end{bmatrix}\]

Any scalar of this matrix will also diagonalize.

## 4.

If a triangular matrix has $n$ distinct values on its trace, those values are its eigenvalues, so it has ll distinct eigenvalues and can therefore be diagonalized.

\[\begin{bmatrix}
  1 & 0 & 0\\
  0 & 2 & 0\\
  0 & 0 & 7\\
\end{bmatrix}\]

## 5.

$A_3 = \m{2 & 0\\2 & 2}$ is not diagonalizable because the repeated eigenvalues have the same eigenvector $\m{0\\1}$, meaning $S$ is not invertible.

## 6.

a. If $A^2 = I$, then $A = A^{-1}$, meaning the eigenvalues must be $\pm 1$.

b. $tr(A) = \pm n$; $\det(A) = -1^n$

## 7.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      4 & 3\\
      1 & 2\\
    \end{bmatrix}
  \end{aligned}
\]

## 8.

a. 

\[
  \begin{aligned}
    & A = uv^T\\
    & Au = uv^tu\\
    & = u \lambda = \lambda u
  \end{aligned}
\]

b. If the first eigenvalue is $u \times v$, the second must be 0 for the trace to sum to that value. 

c. This is a cross-products matrix, so $tr(A) = u_1v_1 + u2v_2 = u \cdot v$

## 9.

```{r, results = 'asis'}
mat2latex(check_commute(m = square("q", "s", "r", "t")))
```
Evidently $qa + sb + rc + td = qa + rc + sb + td$.

So $tr(AB-BA)=0$

## 10.

The eigenvalues of $A^2$ are $1, 4, 8$, so $tr(A^2) = 1 + 4 + 8 = 13$.
For the determinant:

\[
  \begin{aligned}
    & \det(A) = 1 \times 2 \times 4 =8\\
    & \det (A^{-1})^T = \det A^{-1} = 1 /8
  \end{aligned}
\]

## 11.

a. True. Only noninvertible matrices have zero eigenvalues.

b. False. Non-distinct eigenvalues do not guarantee diagonalizability. Consider

\[A = \begin{bmatrix}
  1 & 0 & 0\\
  1 & 1 & 0\\
  1 & 1 & 2\\
\end{bmatrix}\]

for which the eigenspace for $\lambda =1$ has 1 dimension but the algebraic multiplicity is 2.

c. False. If $A$ is already diagonal:

\[ \begin{bmatrix}
  1 & 0 & 0\\
  0 & 1 & 0\\
  0 & 0 & 2\\
\end{bmatrix}\]

Then $S=S^{-1} = I_3$.

## 12.

a. True. If $S$ has rank 1, it is noninvertible and therefore diagonalization is impossible.

b. True. $A$ must have rank 1, since $S$ has rank 1 as well. Since $n$ = 3, two eigenvalues must be 0.

c. False. $A$ itself may be diagonal.

## 13.

## 14.

If $S$ is orthogonal, then:

\[
  \begin{aligned}
    & S \Lambda S^{-1} = S \Lambda S^T\\
    & = (\Lambda S^T)^TS^T\\
    & = S \Lambda^T S^T\\
    & = S \Lambda S^T
  \end{aligned}
\]

## 15.

## 16.

$A^3 = S \Lambda^3 S^{-1}$ and $A^{-1} = S \Lambda^{-1} S^{-1}$.

## 17.

\[
  \begin{aligned}
    & A = \begin{bmatrix}
      1 & 1\\
      0 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
      2 & 0\\
      0 & 5\\
    \end{bmatrix}
    \begin{bmatrix}
      1 & -1\\
      0 & 1\\
    \end{bmatrix}\\
    &A = \begin{bmatrix}
      2 & 3\\
      0 & 5\\
    \end{bmatrix}
  \end{aligned}
\]

## 18.

The eigenvalues are just $\lambda + 2$ and the eigenvectors are the same. So $A + 2I = S (\Lambda +2I)S^{-1}$

\[
  \begin{aligned}
    & = (S \Lambda + 2S)S^{-1}\\
    & = S \Lambda S^{-1} + 2SS^{-1}\\
    & = S \Lambda S^{-1} +2 I\\
    & = A + 2I
  \end{aligned}
\]

## 19.

a. False. Consider a diagonal matrix with zeroes on the diagonal.

b. True. 

c. True.

d. False; invertible matrices do not necessarily have distinct eigenvalues.

## 20.

$S$ is the identity if $A$ is already diagonal.

## 21.


\[
  \begin{aligned}
    & A = \begin{bmatrix}
      4 & 0\\
      1 & 2\\
    \end{bmatrix}\\
     & \Lambda = \begin{bmatrix}
      4 & 0\\
      0 & 2\\
    \end{bmatrix}\\
    & S = \begin{bmatrix}
      2 & -1\\
      1 & 1\\
    \end{bmatrix}\\
    & A =\begin{bmatrix}
      2 & -1\\
      1 & 1\\
    \end{bmatrix} \begin{bmatrix}
      4 & 0\\
      0 & 2\\
    \end{bmatrix}
    \begin{bmatrix}
      1/3 & 1/3\\
      -1/3   & 2/3\\
    \end{bmatrix}
  \end{aligned}
\]

To get $A^{-1}$ we just take the inverse of $\Lambda$, $\m{1/4 & 0 \\ 0 & 1/2}$

## 22.
\[
\begin{bmatrix}
  a & b\\
  b & a
\end{bmatrix}
\]

## 23.

## 24.

\[
  \begin{aligned}
    & \lambda_A = (1, 1)\\
    & \lambda_B = (1, 1)\\
    & \lambda_(A +B) = (3, 1)
  \end{aligned}
\]

## 25.

a. True.

b. False.

c. False.

## 26.

$A$ is noninvertible, and so is $A^2$ The eigenvalues and eigenvectors of $A^2$ are also the same ($1^2 =1$ and $0^2=0$). Both have determinant zero and trace 1. The transposes of each have the same eigenvalues. IF $A$ is diagonalizable, then $A^2 =A$ because $\Lambda^2 = \Lambda$

## 27.

Any values will work. $\m{1\\-1}$ is a possible eigenvector.

## 28.

Given $A = \m{3 & 1\\0 & 3}$, diagonalization is impossible because $A - 3I$ has rank 1, but that eigenvalue has algebraic multiplicity 2. Changing either entry of the diagonal or $(2, 1)$ to a nonzero value would make it diagonalizable.

## 29.

$A^k$ approaches 0 only if if every $\lambda$ is fractional. 

If \[A = \begin{bmatrix}
  .6 & .4\\
  .4 & .6\\
\end{bmatrix}\]
it does not approach 0, because $\lambda_1 = 1$.
\[
  \begin{aligned}
    & \lambda^2 - 1.2 \lambda + .2 = 0 \\
    & (\lambda - 1)(\lambda - .2) = 0
  \end{aligned}
\]

But if \[B = \begin{bmatrix}
  .6 & .9\\
  .1 & .6\\
\end{bmatrix}\]

it does, because 

\[
  \begin{aligned}
    & \lambda^2 - 1.2\lambda + .27 = 0\\
    & (\lambda - .9)(\lambda-.3) = 0
  \end{aligned}
\]

because the lambdas are fractional.

## 30.

For the previous $A$, $\Lambda = \m{1 & 0\\ 0 & .2}$. So

\[S = \begin{bmatrix}
  1 & -1\\
  1 & 1\\
\end{bmatrix}\]

So the factorization is 

\[\begin{bmatrix}
  1 & -1\\
  1 & 1\\
\end{bmatrix} \m{1 & 0\\ 0 & .2}
\begin{bmatrix}
  1/2 & 1/2\\
  -1/2 & 1/2\\
\end{bmatrix}\]

$\Lambda$'s second column gradually zeroes out as $k \rightarrow \infty$.

## 31.

## 32.

Prove a formula for $A^k$.

\[A = \begin{bmatrix}
  2 & 1\\
  1 & 2\\
\end{bmatrix}\]

\[
  \begin{aligned}
    & \Lambda = \begin{bmatrix}
      3 & 0\\
      0 & 1\\
    \end{bmatrix}\\
    & S = \begin{bmatrix}
      1 & -1\\
      1 & 1\\
    \end{bmatrix}\\
    & A =\begin{bmatrix}
      1 & -1\\
      1 & 1\\
    \end{bmatrix} \begin{bmatrix}
      3 & 0\\
      0 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
      1/2 & 1/2\\
      -1/2 & 1/2\\
    \end{bmatrix}
  \end{aligned}
\]

This clearly represents the formula $\frac{1}{2}\m{3^k +1 & 3^k-1\\ 3^k -1 &3^k +1}$

## 33.

Doing the same for $B$, we get

\[
  \begin{aligned}
    & \Lambda = \begin{bmatrix}
      .9 & 0\\
      0 & .3\\
    \end{bmatrix}\\
  & S = \begin{bmatrix}
    3 & -3\\
    1 & 1\\
  \end{bmatrix}\\
  & A = \begin{bmatrix}
    3 & -3\\
    1 & 1\\
  \end{bmatrix}
  \begin{bmatrix}
      .9 & 0\\
      0 & .3\\
    \end{bmatrix}
  \begin{bmatrix}
    1/6 & 1/2\\
    -1/2 & 1/6\\
  \end{bmatrix}
  \end{aligned}
\]

## 34.

If $A$ is diagonalizable, we can prove:
\[
  \begin{aligned}
    & A = S \Lambda S^{-1}\\
    & \det A = \det S \det \Lambda \frac{1}{\det S}\\
    & \det A = \det \Lambda\\
  & \det A = \prod_{i=1}^n \lambda_n
  \end{aligned}
\]

## 35.

$tr(\Lambda) = tr(A)$ because eigenvalues always sum to the trace.

## 36.

## 37.

Proof matrices with the same eigenbasis form a subspace:
\[
  \begin{aligned}
    & S\Lambda_1 S^{-1} + S \Lambda_2S^{-1}\\
    & = S(\Lambda_1S^{-1} + \Lambda_2S^{-1})\\
    & = S(\Lambda_1 + \Lambda_2)S^{-1})\\
    & = S\Lambda_1 S^{-1} + S \Lambda_2S^{-1}\\
  \end{aligned}
\]

Scalar multiples:

\[
  \begin{aligned}
    & kS \Lambda S^{-1} = S(k\Lambda)S^{-1}
  \end{aligned}
\]

If $S =I$, then the subspace is $R^4$.

## 38.

Given $A^2 = A$, $\lambda =1$ belong in the image, with eigenvectors of $A$'s nonzero column vectors, $\lambda =0$ in the kernel ($Ax = 0x \implies Ax = 0)$ with dimension $n - r$. That eigenbasis consists of $e_n$ where row (or column) $n$ of $A$ is all zeroes.

\[
  \begin{aligned}
    & A^2 = A\\
    & A^2 x = Ax\\
    & \lambda^2 x = \lambda x\\
    & x = \lambda x
  \end{aligned}
\]

## 39.

The eigenvectors are _in_ the spaces but do necessarily span them. Eigenvectors may fail to exist, or a multiple eigenvalues may correspond to the same eigenvector.

## 40.

\[
  \begin{aligned}
    & (S \Lambda S^{-1} - \lambda_1I)(S \Lambda S^{-1} - \lambda_2I)\dots (S \Lambda S^{-1} - \lambda_nI) = 0\\
    & S\Lambda^2 S^{-1} - \lambda_1 S \Lambda S^{-1} - \lambda_2 S \Lambda S^{-1} - \lambda_1 \lambda_2I \dots =0\\
    & S \Lambda^n S^{-1} - \prod_{i=0}^n \lambda_i S \Lambda S^{-1} -  \prod_{i=0}^n \lambda_iI = 0\\
    & = S \Lambda^n S^{-1} - \det S \Lambda S^{-1}(S \Lambda S^{-1} + I) = 0\\
    & A^n - (\det A) A - (\det A) I = 0
  \end{aligned}
\]

This is just a generalization of the characteristic polynomial, with $S \Lambda^n S^{-1}$ replacing $-\lambda ^n$

## 41.

Demonstrate Cayley-Hamilton on the Fibonacci matrix $\m{1 & 1\\ 1 & 0}$ $A^2 - A -I = 0$ for this matrix, since its determinant is 1

\[
  \begin{aligned}
    & \begin{bmatrix}
      2 & 1\\
      1 & 1\\
    \end{bmatrix} -
    \begin{bmatrix}
      1 & 1\\
      1 & 0\\
    \end{bmatrix} -
    \begin{bmatrix}
      1 & 0\\
      0 & 1\\
    \end{bmatrix} =0\\
    & \begin{bmatrix}
      1 & 0\\
      0 & 1\\
    \end{bmatrix} -\begin{bmatrix}
      1 & 0\\
      0 & 1\\
    \end{bmatrix} =0\\
    & 0 = 0
  \end{aligned}
\]

## 42.

## 43.

## 44.

If $AB -BA =0$, the matrix representing the equation is singular because it always has the nonzero solution $B = I$.

## 45.

Using the $m \pm \sqrt{m^2 - p}$ trick:

\[ A = \begin{bmatrix}
  .6 & .2\\
  .4 & .8\\
\end{bmatrix}\quad A^{\infty} = \begin{bmatrix}
  1/3 & 1/3\\
  2/3 & 2/3\\
\end{bmatrix}\]

\[
  \begin{aligned}
    & \Lambda = \frac{.6 + .8}{2} \pm \sqrt{\Big( \frac{.6 + .8}{2}\Big)^2 - ((.6)(.8) - (.2)((.4)))}\\
    & =.7 \pm \sqrt{.49 - .4} \\
    & = .7 \pm .3
    S = \begin{bmatrix}
      1/2 & -1\\
      1 & 1\\
    \end{bmatrix}
  \end{aligned}
\]

For $A^{\infty}$:

\[
  \begin{aligned}
    & \lambda = \frac{1}{2} \pm \sqrt{\frac{1}{4} - 0}\\
      & = 1, 0\\
    & S = \begin{bmatrix}
      1/2  & -1\\
      1 & 1\\
    \end{bmatrix}
  \end{aligned}
\]

$A^{100} \approx A^{\infty}$ because $\Lambda^100 \approx \m{1 &0\\0& 0}$, the eigenvalues of $A^{\infty}$ The eigenvalue .4 all but disappears, and $A$ becomes computationally indistinguishable from a one-dimensional matrix.